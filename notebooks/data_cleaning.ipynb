{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ffaa685-1fe8-40b6-bdcf-b7a19fe9aa34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import array\n",
    "from pyspark.sql.functions import concat_ws\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7586890a-fe16-4a52-91c5-2aa432fd191c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean pin data\n",
    "def clean_pin_data(pin_df: pyspark.sql.dataframe.DataFrame):\n",
    "    '''\n",
    "    Cleans the DataFrame that contains information about Pinterest posts.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pin_df : pyspark.sql.dataframe.DataFrame\n",
    "        The DataFrame to be cleaned.\n",
    "        The DataFrame must contain the columns: category, description, downloaded, follower_count, image_src, index, is_image_or_video, poster_name, save_location, tag_list, title, unique_id.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pyspark.sql.dataframe.DataFrame\n",
    "        The cleaned Pinterest post data.\n",
    "    '''\n",
    "    df = pin_df.dropDuplicates().alias('df')\n",
    "    #df = pin_df.alias('df')\n",
    "\n",
    "    # replace several non-values with 'None' in different columns\n",
    "    df = df  \\\n",
    "        .replace('User Info Error', None, ['follower_count', 'poster_name'])  \\\n",
    "        .replace('N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e', None, 'tag_list') \\\n",
    "        .replace('No description available Story format', None, 'description') \\\n",
    "        .replace('Image src error.', None, 'image_src') \\\n",
    "        .replace('No Title Data Available', None, 'title')\n",
    "\n",
    "    # convert all 'k' and 'M' in column 'follower_count' to '000' and '000000' respectively\n",
    "    df = df\\\n",
    "        .withColumn('follower_count', regexp_replace('follower_count', 'k', '000')) \\\n",
    "        .withColumn('follower_count', regexp_replace('follower_count', 'M', '000000'))\n",
    "\n",
    "    # cast 'follower_count' column to integer\n",
    "    df = df.withColumn('follower_count', df.follower_count.cast('int'))\n",
    "    # cast index to int\n",
    "    df = df.withColumn('index', df.index.cast('int'))\n",
    "\n",
    "    # remove the \"Local save in \" bit in the save_location column so that only the path is given\n",
    "    df = df\\\n",
    "        .withColumn('save_location', regexp_replace('save_location', 'Local save in ', ''))\n",
    "    # rename column\n",
    "    df = df.withColumnRenamed('index', 'ind')\n",
    "\n",
    "    # reorder dataframe columns\n",
    "    df = df.select('ind','unique_id','title','description','follower_count','poster_name','tag_list','is_image_or_video','image_src','save_location','category')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "71d4558c-3df3-4e17-a3ee-d70e8ccfcc68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean geo data\n",
    "def clean_geo_data(geo_df: pyspark.sql.dataframe.DataFrame):\n",
    "    '''\n",
    "    Cleans the DataFrame that contains information about geolocation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    geo_df : pyspark.sql.dataframe.DataFrame\n",
    "        The DataFrame to be cleaned.\n",
    "        The DataFrame must contain the columns: country, ind, latitude, longitude, timestamp.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pyspark.sql.dataframe.DataFrame\n",
    "        The cleaned geolocation data.\n",
    "    '''\n",
    "    df = geo_df.dropDuplicates().alias('df')\n",
    "    #df = geo_df.alias('df')\n",
    "\n",
    "    # cast latitude and longitude to 'float', then merge them into an array column called 'coordinates'\n",
    "    df = df\\\n",
    "        .withColumn('latitude', df.latitude.cast('float'))\\\n",
    "        .withColumn('longitude', df.longitude.cast('float'))\n",
    "    df = df\\\n",
    "        .withColumn('coordinates', array(df.latitude, df.longitude))\n",
    "    # cast column named 'timestamp' to the type 'timestamp. cast columnd 'ind' to type 'int'\n",
    "    df = df\\\n",
    "        .withColumn('timestamp', df.timestamp.cast('timestamp'))\\\n",
    "        .withColumn('ind',df.ind.cast('int'))\n",
    "    # drop 'longitude' and 'latitude' columns and reorder the columns\n",
    "    df = df\\\n",
    "        .select('ind', 'country', 'coordinates', 'timestamp')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "f13f9d2d-d5db-46dc-ae5f-6aa424fb223b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Clean user data\n",
    "def clean_user_data(user_df: pyspark.sql.dataframe.DataFrame):\n",
    "    '''\n",
    "    Cleans the DataFrame that contains information about users.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    geo_df : pyspark.sql.dataframe.DataFrame\n",
    "        The DataFrame to be cleaned.\n",
    "        The DataFrame must contain the columns: age, date_joined, first_name, ind, last_name.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pyspark.sql.dataframe.DataFrame\n",
    "        The cleaned users data.\n",
    "    '''\n",
    "\n",
    "    df = user_df.dropDuplicates().alias('df')\n",
    "    #df = user_df.alias('df')\n",
    "\n",
    "    # create column 'user_name' made up by concatenating 'first_name' and 'last_name'\n",
    "    df = df\\\n",
    "        .withColumn('user_name', concat_ws(' ', df.first_name, df.last_name))\n",
    "\n",
    "    # cast 'ind' and 'age' to an 'int' type, and date_joined' to a 'timestamp' type.\n",
    "    df = df\\\n",
    "        .withColumn('date_joined', df.date_joined.cast('timestamp'))\\\n",
    "        .withColumn('ind', df.ind.cast('int'))\\\n",
    "        .withColumn('age', df.age.cast('int'))\n",
    "\n",
    "    # drop 'first_name' and 'last_name', and reorder the columns\n",
    "    df = df\\\n",
    "        .select('ind', 'user_name', 'age', 'date_joined')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68e56bac-cc9b-49e3-a1cb-fa00be8656d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run /Users/joelcosta94i@gmail.com/data_extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec243bc6-838c-470c-9036-094b80b80dac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load cleaned data\n",
    "# Load the data, then clean it, and return the cleaned data\n",
    "def load_cleaned_data():\n",
    "    '''\n",
    "    Loads the data from the S3 bucket, cleans it, then returns it as a triple of dataframes\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df_pin : pyspark.sql.dataframe.DataFrame\n",
    "        The dataframe containing the cleaned pin data.\n",
    "    df_geo : pyspark.sql.dataframe.DataFrame\n",
    "        The dataframe containing the cleaned geo data.\n",
    "    df_user : pyspark.sql.dataframe.DataFrame\n",
    "        The dataframe containing the cleaned user data.\n",
    "    '''\n",
    "    df_pin = create_df(\"0a6a638f5991.pin\")\n",
    "    df_geo = create_df(\"0a6a638f5991.geo\")\n",
    "    df_user = create_df(\"0a6a638f5991.user\")\n",
    "    df_pin = clean_pin_data(df_pin)\n",
    "    df_geo = clean_geo_data(df_geo)\n",
    "    df_user = clean_user_data(df_user)\n",
    "    return df_pin, df_geo, df_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1c0b677-7515-4177-88c0-867e852f47b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_cleaning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
