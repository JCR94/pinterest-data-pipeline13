{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91313165-6c7b-4e48-8a30-df7840d9def2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create single DataFrame\n",
    "def create_df(topic: str):\n",
    "    '''\n",
    "    Creates a dataframe from the data in the S3 bucket from a given topic.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topic: str\n",
    "        The name of the Kafka topic, which coincides with the name of the directory inside bucket-name/topics.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pyspark.sql.dataframe.DataFrame\n",
    "        The dataframe containing the data stored in the bucket.\n",
    "    '''\n",
    "    file_location = \"/mnt/jc_bucket/topics/\" + topic + \"/partition=0/*.json\" \n",
    "    file_type = \"json\"\n",
    "    # Ask Spark to infer the schema\n",
    "    infer_schema = \"true\"\n",
    "    # Read in JSONs from mounted S3 bucket\n",
    "    df = spark.read.format(file_type) \\\n",
    "    .option(\"inferSchema\", infer_schema) \\\n",
    "    .load(file_location)\n",
    "    # Display Spark dataframe to check its content\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14c36db4-ceb3-4495-a783-e923c716e0db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_extraction",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
